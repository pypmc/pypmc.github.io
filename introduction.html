
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Overview &#8212; pypmc 1.1.4 documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="2. Installation" href="installation.html" />
    <link rel="prev" title="pypmc" href="index.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="installation.html" title="2. Installation"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="index.html" title="pypmc"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">pypmc 1.1.4 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">1. </span>Overview</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="overview">
<h1><span class="section-number">1. </span>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h1>
<p>Let <span class="math notranslate nohighlight">\(P\)</span> denote the target target density, and let <span class="math notranslate nohighlight">\(q\)</span>
equal the proposal density. Then the basic idea of importance sampling
is to approximate the integral of <span class="math notranslate nohighlight">\(P\)</span> as</p>
<div class="math notranslate nohighlight">
\[\int \mbox{d} x \, P(x) = \int \mbox{d} x \, q(x) \frac{P(x)}{q(x)}
\approx \frac{1}{N} \sum_{i=1}{N} \frac{P(x_i)}{q(x_i)}\]</div>
<p>where each <span class="math notranslate nohighlight">\(x\)</span> is a <span class="math notranslate nohighlight">\(D\)</span>-dimensional vector drawn
independently from <span class="math notranslate nohighlight">\(q\)</span>. The <span class="math notranslate nohighlight">\(i\)</span>-th importance weight is
defined as</p>
<div class="math notranslate nohighlight">
\[w_i \equiv \frac{P(x_i)}{q(x_i)}\]</div>
<p>The most accurate estimate is obtained for <span class="math notranslate nohighlight">\(q=P\)</span>, so the goal is
make <span class="math notranslate nohighlight">\(q\)</span> as close as possible to <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p>In pypmc, we choose <span class="math notranslate nohighlight">\(q\)</span> to be a mixture density composed of
either Gaussian or student’s t components <span class="math notranslate nohighlight">\(q_j\)</span></p>
<div class="math notranslate nohighlight">
\[q(x) = \sum_j \alpha_j q_j(x), \: \sum_j \alpha_j = 1 \,.\]</div>
<div class="section" id="initial-proposal-density">
<h2><span class="section-number">1.1. </span>Initial proposal density<a class="headerlink" href="#initial-proposal-density" title="Permalink to this headline">¶</a></h2>
<p>The key ingredient to make adaptive importance sampling work is a good
initial proposal density that closely resembles the target density. A
general method to automatically determine the bulk of the target is to
run multiple Markov chains, and to use clustering to extract a mixture
density from the samples <a class="reference internal" href="references.html#bc13" id="id1"><span>[BC13]</span></a>. We provide a generic implementation
of adaptive local-random-walk MCMC <a class="reference internal" href="references.html#hst01" id="id2"><span>[HST01]</span></a> featuring Gauss and
student’s t local proposals. MCMC can be used standalone and is
usually all one needs for a unimodal distribution if the evidence is
not of interest. For the clustering, we offer several options. At the
level of individual samples, we have</p>
<ul class="simple">
<li><p>population Monte Carlo <a class="reference internal" href="references.html#cap-08" id="id3"><span>[Cap+08]</span></a></p></li>
<li><p>variational Bayes for Gaussian mixtures <a class="reference internal" href="references.html#bis06" id="id4"><span>[Bis06]</span></a></p></li>
</ul>
<p>and at the level of Gaussian mixtures, there is</p>
<ul class="simple">
<li><p>hierarchical clustering <a class="reference internal" href="references.html#gr04" id="id5"><span>[GR04]</span></a> as suggested by Beaujean &amp; Caldwell
<a class="reference internal" href="references.html#bc13" id="id6"><span>[BC13]</span></a></p></li>
<li><p>variational Bayes (VBmix) <a class="reference internal" href="references.html#bgp10" id="id7"><span>[BGP10]</span></a></p></li>
</ul>
</div>
<div class="section" id="proposal-updates">
<h2><span class="section-number">1.2. </span>Proposal updates<a class="headerlink" href="#proposal-updates" title="Permalink to this headline">¶</a></h2>
<p>Starting with an initial proposal, samples are drawn from the proposal
<span class="math notranslate nohighlight">\(q\)</span>, the importance weights are computed, and the proposal is
updated using the samples and weights to more closely approximate the
target density. The two main update algorithms included are:</p>
<ul class="simple">
<li><p>Population Monte Carlo (PMC)</p></li>
<li><p>Variational Bayes (VB)</p></li>
</ul>
</div>
<div class="section" id="pmc">
<h2><span class="section-number">1.3. </span>PMC<a class="headerlink" href="#pmc" title="Permalink to this headline">¶</a></h2>
<p>Based on the original proposal by Cappé et al. <a class="reference internal" href="references.html#cap-08" id="id8"><span>[Cap+08]</span></a>, we offer
updates for a mixture of Gaussian or student’s t components. Important
improvements are:</p>
<ul class="simple">
<li><p>The option to adapt the student’s t degree of freedom - individually
for each component - as in <a class="reference internal" href="references.html#hod12" id="id9"><span>[HOD12]</span></a>. That’s one less parameter that
the user has to guess.</p></li>
<li><p>The power to combine the proposals of subsequent steps
<a class="reference internal" href="references.html#cor-12" id="id10"><span>[Cor+12]</span></a>. This increases the effective sample size per wallclock
time and helps in reducing undesired samples with very large weight — <em>outliers</em> —
that adversely affect the variance of the integral estimate.</p></li>
</ul>
</div>
<div class="section" id="variational-bayes">
<h2><span class="section-number">1.4. </span>Variational Bayes<a class="headerlink" href="#variational-bayes" title="Permalink to this headline">¶</a></h2>
<p>A powerful alternative to PMC is to use the variational Bayes
algorithm <a class="reference internal" href="references.html#bis06" id="id11"><span>[Bis06]</span></a> to fit a Gaussian mixture to samples. We include a
variant that also works with importance-weighted samples. Our
implementation allows the user to set all values of the
prior/posterior hyperparameters. Variational Bayes can therefore be
used in a sequential manner to incrementally update the knowledge
about the Gaussian mixture as new (importance) samples arrive.</p>
</div>
<div class="section" id="performance">
<h2><span class="section-number">1.5. </span>Performance<a class="headerlink" href="#performance" title="Permalink to this headline">¶</a></h2>
<p>Importance sampling naturally lends itself to massive parallelization
because once the samples are drawn from the proposal, the computation
of N importance weights requires N independent calls to the target
density. Even for moderately complicated problems, these calls are
typically the most expensive part of the calculation. With pypmc, the
importance weights can optionally be computed in multiple processes on
a single machine or a whole cluster with mpi4py. Similarly, multiple
Markov chains are independent and can be run in separate processes.</p>
<p>The second major contribution to overall computing time is the update
algorithm itself. We profiled the program and transferred the relevant
loops from python to compiled C code via cython.</p>
<p>The code is designed such that it does not get into the users way;
full control over how individual components interact is a major design
goal.</p>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">1. Overview</a><ul>
<li><a class="reference internal" href="#initial-proposal-density">1.1. Initial proposal density</a></li>
<li><a class="reference internal" href="#proposal-updates">1.2. Proposal updates</a></li>
<li><a class="reference internal" href="#pmc">1.3. PMC</a></li>
<li><a class="reference internal" href="#variational-bayes">1.4. Variational Bayes</a></li>
<li><a class="reference internal" href="#performance">1.5. Performance</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="index.html"
                        title="previous chapter">pypmc</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="installation.html"
                        title="next chapter"><span class="section-number">2. </span>Installation</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/introduction.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="installation.html" title="2. Installation"
             >next</a> |</li>
        <li class="right" >
          <a href="index.html" title="pypmc"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">pypmc 1.1.4 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">1. </span>Overview</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2019, Frederik Beaujean and Stephan Jahn.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.5.2.
    </div>
  </body>
</html>